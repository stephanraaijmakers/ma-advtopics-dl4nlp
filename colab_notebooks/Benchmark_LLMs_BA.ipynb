{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D8CauuB5_vn"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.evaluation import QAEvalChain\n",
        "from langchain.llms import HuggingFacePipeline # for HF models\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "5D6ysmCa6XQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "bZE-wOrA7pSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "KfMVdpn96fcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "p3fyAQK07iEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"Pick the right answer from A/B/C. Only reply with A, B, or C.\n",
        "\n",
        "    {question}\"\"\"\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "id": "kV2yCQtV6nQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "TuLo_Qjn7TfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may need to apply for access to the HF model you choose."
      ],
      "metadata": {
        "id": "3gsVzceIT8Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/flan-t5-large\"  # Replace with your desired model ID\n",
        "pipe = pipeline(model=model_id, device=0 if os.environ.get(\"CUDA_VISIBLE_DEVICES\") else -1, token=HF_TOKEN) # Use GPU if available\n",
        "llm_hf = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "VHczCR8fPtss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=llm_hf, # choose llm_openai or llm_hf (for Huggingface models)\n",
        "    prompt=prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "-zgNlk3d6ycT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = QAEvalChain.from_llm(llm_hf, chain_type=\"stuff\") # choose llm_hf or llm_openai"
      ],
      "metadata": {
        "id": "6n8A9Y368ZB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    {\"question\": \"What is the capital of France? A:Berlin B:Paris C:Amsterdam\", \"expected_answer\": \"B\"},\n",
        "    {\"question\": \"Who wrote '1984'? A:Shakespeare B:Franzen C:George Orwell\", \"expected_answer\": \"C\"},\n",
        "    {\"question\": \"What is the chemical symbol for water? A:H2O B:NO C:Mg\", \"expected_answer\": \"A\"},\n",
        "]"
      ],
      "metadata": {
        "id": "FK5132OV8lLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for example in test_data:\n",
        "    predicted_answer = llm_chain.run(example[\"question\"])  # Run the LLM to get the prediction\n",
        "    # Clean up the prediction to ensure it's just A, B, or C\n",
        "    print(predicted_answer)\n",
        "    predicted_answer = predicted_answer.strip().upper()  # Strip whitespace and make uppercase\n",
        "\n",
        "    # Enforce A, B, or C output using regex\n",
        "\n",
        "    match = re.search(r\"[ABC]\", predicted_answer)\n",
        "    if match:\n",
        "        predicted_answer = match.group(0)\n",
        "    else:\n",
        "        predicted_answer = \"A\"  # Default to A if no match, consider handling better\n",
        "\n",
        "    predictions.append({\"question\": example[\"question\"], \"generated_answer\": predicted_answer})\n",
        "\n"
      ],
      "metadata": {
        "id": "oLH87KmqCwDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "TVvCiNe7C_tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = evaluator.evaluate(\n",
        "    examples=test_data,\n",
        "    predictions=predictions,\n",
        "    question_key=\"question\",\n",
        "    answer_key=\"expected_answer\",\n",
        "    prediction_key=\"generated_answer\"\n",
        ")"
      ],
      "metadata": {
        "id": "kv99DVUT82UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "LPASP4oSBrDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(outputs):\n",
        "    # The 'results' key contains a string like 'CORRECT' or 'INCORRECT'\n",
        "    is_correct_str = item.get('results', 'N/A')\n",
        "\n",
        "    # You can convert it to a boolean if needed\n",
        "    is_correct = is_correct_str == 'CORRECT'\n",
        "    question = test_data[i]['question']\n",
        "    expected_answer = test_data[i]['expected_answer']\n",
        "\n",
        "    # Get the predicted answer from the predictions list used in evaluate()\n",
        "    # Access the 'generated_answer' from your predictions list.\n",
        "    predicted_answer = predictions[i]['generated_answer']\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Expected: {expected_answer}\")\n",
        "    print(f\"Predicted: {predicted_answer}\")\n",
        "    print(f\"Correct: {is_correct}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "Dekzongm9Nan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NzIHCIqkHsOa"
      }
    }
  ]
}